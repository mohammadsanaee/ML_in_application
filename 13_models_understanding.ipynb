{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mohammadsanaee/ML_in_application/blob/main/13_models_understanding.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!rm -rf harbour-space-text-mining-course\n",
        "!git clone https://github.com/horoshenkih/harbour-space-text-mining-course.git\n",
        "import sys\n",
        "sys.path.append('harbour-space-text-mining-course')\n",
        "!pip install -q catboost\n",
        "!pip install -q shap\n",
        "\n",
        "from tmcourse.utils import (\n",
        "    display_token_importance,\n",
        ")\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from tqdm.notebook import tqdm\n",
        "from IPython.display import HTML, display\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "sns.set(rc={'figure.figsize':(15,10)})"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dqzxKAi8El1d",
        "outputId": "eb71e32d-e4b2-4ddd-fd6d-81dc504ba8ac"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'harbour-space-text-mining-course'...\n",
            "remote: Enumerating objects: 738, done.\u001b[K\n",
            "remote: Counting objects: 100% (175/175), done.\u001b[K\n",
            "remote: Compressing objects: 100% (105/105), done.\u001b[K\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Linear models: feature effect"
      ],
      "metadata": {
        "id": "Hq9veZPWvMhh"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2JY_I7vYvCbl"
      },
      "outputs": [],
      "source": [
        "from sklearn.datasets import fetch_20newsgroups\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "categories = [\n",
        "    \"alt.atheism\",\n",
        "    \"talk.religion.misc\",\n",
        "    \"comp.graphics\",\n",
        "    \"sci.space\",\n",
        "]\n",
        "remove = (\"headers\", \"footers\", \"quotes\")\n",
        "\n",
        "data_train = fetch_20newsgroups(\n",
        "    subset=\"train\",\n",
        "    categories=categories,\n",
        "    shuffle=True,\n",
        "    random_state=42,\n",
        "    remove=remove,\n",
        ")\n",
        "\n",
        "data_test = fetch_20newsgroups(\n",
        "    subset=\"test\",\n",
        "    categories=categories,\n",
        "    shuffle=True,\n",
        "    random_state=42,\n",
        "    remove=remove,\n",
        ")\n",
        "\n",
        "# order of labels in `target_names` can be different from `categories`\n",
        "target_names = data_train.target_names\n",
        "\n",
        "# split target in a training set and a test set\n",
        "y_train, y_test = data_train.target, data_test.target\n",
        "\n",
        "# Extracting features from the training data using a sparse vectorizer\n",
        "\n",
        "vectorizer = TfidfVectorizer(\n",
        "    sublinear_tf=True, max_df=0.5, min_df=5, stop_words=\"english\"\n",
        ")\n",
        "X_train = vectorizer.fit_transform(data_train.data)\n",
        "\n",
        "\n",
        "# Extracting features from the test data using the same vectorizer\n",
        "X_test = vectorizer.transform(data_test.data)\n",
        "\n",
        "feature_names = vectorizer.get_feature_names_out()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.linear_model import RidgeClassifier\n",
        "\n",
        "clf = RidgeClassifier(tol=1e-2, solver=\"sparse_cg\")\n",
        "clf.fit(X_train, y_train)\n",
        "pred = clf.predict(X_test)"
      ],
      "metadata": {
        "id": "DjwnqD2hoqDO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# learned coefficients weighted by frequency of appearance\n",
        "average_feature_effects = clf.coef_ * np.asarray(X_train.mean(axis=0)).ravel()\n",
        "\n",
        "for i, label in enumerate(target_names):\n",
        "    top5 = np.argsort(average_feature_effects[i])[-5:][::-1]\n",
        "    if i == 0:\n",
        "        top = pd.DataFrame(feature_names[top5], columns=[label])\n",
        "        top_indices = top5\n",
        "    else:\n",
        "        top[label] = feature_names[top5]\n",
        "        top_indices = np.concatenate((top_indices, top5), axis=None)\n",
        "top_indices = np.unique(top_indices)\n",
        "predictive_words = feature_names[top_indices]\n",
        "\n",
        "# plot feature effects\n",
        "bar_size = 0.25\n",
        "padding = 0.75\n",
        "y_locs = np.arange(len(top_indices)) * (4 * bar_size + padding)\n",
        "\n",
        "fig, ax = plt.subplots(figsize=(10, 8))\n",
        "for i, label in enumerate(target_names):\n",
        "    ax.barh(\n",
        "        y_locs + (i - 2) * bar_size,\n",
        "        average_feature_effects[i, top_indices],\n",
        "        height=bar_size,\n",
        "        label=label,\n",
        "    )\n",
        "ax.set(\n",
        "    yticks=y_locs,\n",
        "    yticklabels=predictive_words,\n",
        "    ylim=[\n",
        "        0 - 4 * bar_size,\n",
        "        len(top_indices) * (4 * bar_size + padding) - 4 * bar_size,\n",
        "    ],\n",
        ")\n",
        "ax.legend(loc=\"lower right\")\n",
        "\n",
        "print(\"top 5 keywords per class:\")\n",
        "print(top)"
      ],
      "metadata": {
        "id": "syfm5E0Zof8C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Feature importance for Random Forest"
      ],
      "metadata": {
        "id": "-Rugirvao5aU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We generate a synthetic dataset with only 3 informative features."
      ],
      "metadata": {
        "id": "7ASo-PlMpYFn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import make_classification\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "X, y = make_classification(\n",
        "    n_samples=1000,\n",
        "    n_features=10,\n",
        "    n_informative=3,\n",
        "    n_redundant=0,\n",
        "    n_repeated=0,\n",
        "    n_classes=2,\n",
        "    random_state=0,\n",
        "    shuffle=False,\n",
        ")\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42)"
      ],
      "metadata": {
        "id": "YQakSiGmpXwB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.ensemble import RandomForestClassifier\n",
        "\n",
        "feature_names = [f\"feature {i}\" for i in range(X.shape[1])]\n",
        "forest = RandomForestClassifier(random_state=0)\n",
        "forest.fit(X_train, y_train)"
      ],
      "metadata": {
        "id": "WEI8w08dpCcp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Feature importances are provided by the fitted attribute feature_importances_ and they are computed as the mean and standard deviation of accumulation of the impurity decrease within each tree."
      ],
      "metadata": {
        "id": "qwUSitaCpraI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "importances = forest.feature_importances_\n",
        "std = np.std([tree.feature_importances_ for tree in forest.estimators_], axis=0)"
      ],
      "metadata": {
        "id": "Zq6vVBTbpm1j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Letâ€™s plot the impurity-based importance."
      ],
      "metadata": {
        "id": "fVfr-ieHp0KJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "forest_importances = pd.Series(importances, index=feature_names)\n",
        "\n",
        "fig, ax = plt.subplots()\n",
        "forest_importances.plot.bar(yerr=std, ax=ax)\n",
        "ax.set_title(\"Feature importances using MDI\")\n",
        "ax.set_ylabel(\"Mean decrease in impurity\")\n",
        "fig.tight_layout()"
      ],
      "metadata": {
        "id": "MntWRHLdpup_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Permutation importance"
      ],
      "metadata": {
        "id": "Auz_UbZ8p4z7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Permutation feature importance overcomes limitations of the impurity-based feature importance: they do not have a bias toward high-cardinality features and can be computed on a left-out test set."
      ],
      "metadata": {
        "id": "JOCSUBrUp-sM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.inspection import permutation_importance\n",
        "\n",
        "result = permutation_importance(\n",
        "    forest, X_test, y_test, n_repeats=10, random_state=42, n_jobs=2\n",
        ")\n",
        "\n",
        "forest_importances = pd.Series(result.importances_mean, index=feature_names)"
      ],
      "metadata": {
        "id": "gzPaa4GJp9yi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "fig, ax = plt.subplots()\n",
        "forest_importances.plot.bar(yerr=result.importances_std, ax=ax)\n",
        "ax.set_title(\"Feature importances using permutation on full model\")\n",
        "ax.set_ylabel(\"Mean accuracy decrease\")\n",
        "fig.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "g1ZoXEJcqLLB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "As we see, permutation importance gives lower weights to random high-cardinality features."
      ],
      "metadata": {
        "id": "1TpBx3xfrERx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# CatBoost: feature importance"
      ],
      "metadata": {
        "id": "l-8Tm6T3rXR-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from catboost import CatBoostClassifier, Pool\n",
        "\n",
        "clf = CatBoostClassifier()\n",
        "clf.fit(X_train, y_train, verbose=False)"
      ],
      "metadata": {
        "id": "bNkjiKQirZ3r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "feature_importances = clf.get_feature_importance(\n",
        "    Pool(X_test, y_test, feature_names=feature_names),\n",
        "    prettified=True,\n",
        ")\n",
        "feature_importances"
      ],
      "metadata": {
        "id": "jKodH8WZsZqz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def visualize_feature_importances(clf, pool):\n",
        "    feature_importances = clf.get_feature_importance(pool, prettified=True)\n",
        "\n",
        "    sns.barplot(x=\"Importances\", y=\"Feature Id\", data=feature_importances)\n",
        "\n",
        "visualize_feature_importances(\n",
        "    clf,\n",
        "    Pool(X_test, y_test, feature_names=feature_names)\n",
        ")"
      ],
      "metadata": {
        "id": "_DSTV0TKswoi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# CatBoost: feature interactions"
      ],
      "metadata": {
        "id": "-TYZD7gFtfm7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def visualize_feature_interactions(clf, pool, feature_names):\n",
        "    feature_interactions = clf.get_feature_importance(pool, type=\"Interaction\")\n",
        "\n",
        "    interactions = []\n",
        "    feature_pairs = []\n",
        "    for fi in feature_interactions:\n",
        "        interactions.append(fi[2])\n",
        "        feature_pairs.append(feature_names[int(fi[0])] + '__' + feature_names[int(fi[1])])\n",
        "    feature_interactions_df = pd.DataFrame({\n",
        "        'interaction': interactions,\n",
        "        'feature_pair': feature_pairs,\n",
        "    })\n",
        "\n",
        "    sns.barplot(x=\"interaction\", y=\"feature_pair\", data=feature_interactions_df)\n",
        "\n",
        "visualize_feature_interactions(\n",
        "    clf,\n",
        "    Pool(X_test, y_test, feature_names=feature_names),\n",
        "    feature_names\n",
        ")"
      ],
      "metadata": {
        "id": "vW-a8LC2tms1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# SHAP values"
      ],
      "metadata": {
        "id": "6YGDpBsmuIPS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Get the data"
      ],
      "metadata": {
        "id": "JiZx0GHvu1VS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.read_csv('https://raw.githubusercontent.com/horoshenkih/harbour-space-ds210/master/datasets/california_housing_prices.csv')\n",
        "df.head()"
      ],
      "metadata": {
        "id": "o7d48Pf0u2pI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X = df.drop('median_house_value', axis=1)\n",
        "y = df.median_house_value\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=42)"
      ],
      "metadata": {
        "id": "vRqqiGgdu3qx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from catboost import CatBoostRegressor\n",
        "clf = CatBoostRegressor()\n",
        "clf.fit(X_train, y_train, cat_features=['ocean_proximity'], verbose=False, plot=False)"
      ],
      "metadata": {
        "id": "UYusYenCu9I_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "SHAP algorithm is implemented in `shap` package.\n",
        "\n",
        "It works witn CatBoost and supports categorical features."
      ],
      "metadata": {
        "id": "IGBWehHQu0iH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import shap"
      ],
      "metadata": {
        "id": "_sQRVTN-uVbt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "explainer = shap.TreeExplainer(clf)\n",
        "pool_test = Pool(X_test, y_test, cat_features=['ocean_proximity'])\n",
        "shap_values = explainer.shap_values(pool_test)"
      ],
      "metadata": {
        "id": "n1VqoeO-vzVS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# summarize the effects of all the features\n",
        "shap.summary_plot(shap_values, X_test)"
      ],
      "metadata": {
        "id": "6mNgktlpv5ad"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Look at an individual sample"
      ],
      "metadata": {
        "id": "V1xmYu9XxABw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "shap.initjs()\n",
        "\n",
        "sample_idx = 1000\n",
        "shap.force_plot(explainer.expected_value, shap_values[sample_idx,:], X_test.iloc[sample_idx,:])"
      ],
      "metadata": {
        "id": "Pm_P7GvkxCHz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Masking and occlusion"
      ],
      "metadata": {
        "id": "8PFrSIMwDyUC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import fetch_20newsgroups\n",
        "\n",
        "dataset = fetch_20newsgroups(\n",
        "    subset=\"all\",\n",
        "    remove=('headers', 'footers', 'quotes'),\n",
        "    categories=(\"sci.space\", \"rec.autos\", \"talk.politics.misc\", \"comp.graphics\")\n",
        ")\n",
        "\n",
        "texts = dataset.data\n",
        "y = dataset.target"
      ],
      "metadata": {
        "id": "_8H6CNanELw0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "texts_train, texts_test, y_train, y_test = train_test_split(texts, y, test_size=0.5, random_state=1)"
      ],
      "metadata": {
        "id": "22eX48FPEgMB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.linear_model import SGDClassifier\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.pipeline import Pipeline\n",
        "\n",
        "clf = Pipeline([\n",
        "    (\"vec\", TfidfVectorizer(stop_words=\"english\")),\n",
        "    (\"clf\", SGDClassifier(alpha=.0001, max_iter=50, loss=\"log\", random_state=0)),\n",
        "])\n",
        "\n",
        "clf.fit(texts_train, y_train)"
      ],
      "metadata": {
        "id": "rjcAOCRGEyiR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def compute_token_importance(clf, text):\n",
        "    from scipy.linalg import norm\n",
        "    # a simple tokenizer: split by whitespace\n",
        "    # not necessarily the same tokenizer that the vectorizer uses!\n",
        "    import re\n",
        "    tokens = re.split(r'\\W+', text.strip())\n",
        "    initial_text = \" \".join(tokens)\n",
        "    # predict the probabilities of all classes for the initial text\n",
        "    initial_class_distribution = clf.predict_proba([initial_text])[0]\n",
        "    token_importances = []\n",
        "    for i, token in enumerate(tokens):\n",
        "        masked_tokens = tokens[:]\n",
        "        # mask (remove) one token\n",
        "        masked_tokens[i] = \"<UNKNOWN>\"\n",
        "        masked_text = \" \".join(masked_tokens)\n",
        "        # classify the text with masked token\n",
        "        masked_class_distribution = clf.predict_proba([masked_text])[0]\n",
        "        # token importance is the distance between\n",
        "        # the initial distribution of classes\n",
        "        # and the distribution after removal of the token\n",
        "        token_importance = norm(initial_class_distribution - masked_class_distribution)\n",
        "        token_importances.append((token, token_importance))\n",
        "    return token_importances\n",
        "\n",
        "# remember the initial categories in the dataset\n",
        "print(\"categories:\", \", \".join(dataset.target_names))\n",
        "for text, label in zip(texts_test[:20], y_test[:20]):\n",
        "    display(HTML(\"<hr>\"))\n",
        "    display(HTML(dataset.target_names[label]))  # uncomment this line to see the true label\n",
        "    token_importances = compute_token_importance(clf, text)\n",
        "    display_token_importance(token_importances)\n",
        "    display(HTML(\"<hr>\"))"
      ],
      "metadata": {
        "id": "v89oFH_wE69Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Visualizing layer activations in ConvNets"
      ],
      "metadata": {
        "id": "98M_Se5iH9rO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Get images of some cute animals."
      ],
      "metadata": {
        "id": "7YD9gsV7x8NE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torchvision"
      ],
      "metadata": {
        "id": "duI6TvJzVSzQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data = torchvision.datasets.OxfordIIITPet(\n",
        "    root=\"./data\",\n",
        "    download=True,\n",
        "    transform=torchvision.transforms.ToTensor(),\n",
        ")"
      ],
      "metadata": {
        "id": "-jICb-4kYFl1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import torchvision.transforms.functional as F\n",
        "import numpy as np\n",
        "\n",
        "image_index = 31\n",
        "img = data[image_index][0]\n",
        "plt.imshow(np.asarray(F.to_pil_image(img)))"
      ],
      "metadata": {
        "id": "7zfU7oSeY28M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Load pre-trained ResNet model and check that it works as expected"
      ],
      "metadata": {
        "id": "5zEQ02fGyAHC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from torchvision.io import read_image\n",
        "from torchvision.models import resnet50, ResNet50_Weights\n",
        "\n",
        "# Step 1: Initialize model with the best available weights\n",
        "weights = ResNet50_Weights.DEFAULT\n",
        "model = resnet50(weights=weights)\n",
        "model.eval()\n",
        "\n",
        "# Step 2: Initialize the inference transforms\n",
        "preprocess = weights.transforms()\n",
        "\n",
        "# Step 3: Apply inference preprocessing transforms\n",
        "batch = preprocess(img).unsqueeze(0)\n",
        "\n",
        "# Step 4: Use the model and print the predicted category\n",
        "prediction = model(batch).squeeze(0).softmax(0)\n",
        "class_id = prediction.argmax().item()\n",
        "score = prediction[class_id].item()\n",
        "category_name = weights.meta[\"categories\"][class_id]\n",
        "print(f\"{category_name}: {100 * score:.1f}%\")"
      ],
      "metadata": {
        "id": "zzyyfEDJVUri"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "PyTorch provides tools for model inspection: `get_graph_node_names` and `create_feature_extractor`"
      ],
      "metadata": {
        "id": "bthFo23XyGc3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from torchvision.models.feature_extraction import get_graph_node_names, create_feature_extractor\n",
        "train_nodes, eval_nodes = get_graph_node_names(model)\n",
        "eval_nodes[:10]"
      ],
      "metadata": {
        "id": "JGQq9iWshXJ9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Extract the output of the first ReLU. Check that there are sparse activations."
      ],
      "metadata": {
        "id": "yzn6AeiDyRwx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "extract_relu = create_feature_extractor(model, {'relu': 'layer1'})\n",
        "img2 = extract_relu(batch)['layer1']\n",
        "\n",
        "fig, ax = plt.subplots(8, 8, figsize=(15,15))\n",
        "ax = ax.ravel()\n",
        "for i in range(img2.shape[1]):\n",
        "    ax[i].imshow(img2[0][i].detach().numpy())\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "FhfU00OBbYDo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Compare with uninitialized model - without training, all activations are dense"
      ],
      "metadata": {
        "id": "pFwTruvMydY7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "extract_relu = create_feature_extractor(resnet50(), {'relu': 'layer1'})\n",
        "img2 = extract_relu(batch)['layer1']\n",
        "\n",
        "fig, ax = plt.subplots(8, 8, figsize=(15,15))\n",
        "ax = ax.ravel()\n",
        "for i in range(img2.shape[1]):\n",
        "    ax[i].imshow(img2[0][i].detach().numpy())\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "DbLhAcrHmukO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Home Assignment 10\n",
        "\n",
        "In this assignment, you will work with probably the most famous and well-studied dataset.\n",
        "\n",
        "Your task is to predict the value of `Survived` column (binary classification).\n",
        "\n",
        "You can read more about the data here: https://www.kaggle.com/c/titanic/data"
      ],
      "metadata": {
        "id": "nvcIch1ryva5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.read_csv(\"https://raw.githubusercontent.com/datasciencedojo/datasets/master/titanic.csv\")\n",
        "df.head()"
      ],
      "metadata": {
        "id": "c2UvVfybzFuF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Exercise 1 (2 points)\n",
        "\n",
        "Create training dataset, train CatBoost model and **find feature importances** for all features. You need to decide which columns to use as features. Feel free to do feature engineering."
      ],
      "metadata": {
        "id": "4nav6n960RD_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# YOUR CODE HERE"
      ],
      "metadata": {
        "id": "i5TtOxZz1D8S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Exercise 2 (2 points)\n",
        "\n",
        "Compute feature interactions.\n",
        "\n"
      ],
      "metadata": {
        "id": "dre0il8g0lT8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# YOUR CODE HERE"
      ],
      "metadata": {
        "id": "QLhrFjpO1DQe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Exercise 3 (2 points)\n",
        "\n",
        "Compute SHAP values. Use `shap.summary_plot`. Interpret the results."
      ],
      "metadata": {
        "id": "KN4X6xsx01BA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# YOUR CODE HERE"
      ],
      "metadata": {
        "id": "-7iuW_od1BBS"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}